S3static sync tests
===================

We have a same directory and a test bucket (in a faux s3).

    >>> import os, mock
    >>> os.mkdir('sample')
    >>> mkfile('sample/f1')
    >>> mkfile('sample/d1/f1')
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')

Cuz we're mean, we'll create a broken symlink:

    >>> os.symlink('lose', 'sample/d1/hahaha')

Later:

    >>> now += 3600

We'll sync to our faux bucket:

    >>> import zc.s3staticsync

    >>> now += 30
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    ... # doctest: +ELLIPSIS
    bad file 'd1/hahaha'
    Traceback (most recent call last):
    ...
    OSError: [Errno 2] No such file or directory: '.../sample/d1/hahaha'

Note that we logged an exception for the bad link, but we kept going.

    >>> os.remove('sample/d1/hahaha')

Let's check what we have in our bucket:

    >>> import boto.s3.connection
    >>> s3 = boto.s3.connection.S3Connection()
    >>> bucket = s3.get_bucket('test')

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f2
    d1/f1
    d1/f2
    f1
    f2

We did 6 puts and no deletes:

    >>> bucket.puts, bucket.deletes
    (6, 0)

Time passes and we so it all again:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

Nothing should have been put or deleted:

    >>> bucket.puts, bucket.deletes
    (6, 0)


 Let's make some changes and make sure they're reflected:

    >>> now += 1000
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')

    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3

    >>> bucket.puts, bucket.deletes
    (10, 3)

There's a kinda weird case.  We compare file modification time to s3
modification time. To account for that, and for clocks being out of
sync, we add a fudge factor to the modification time. This causes
newly modified files to be uploaded twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

But only twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

Emptying a bucket
=================

An easy way to empty a bucket is to sync from an empty directory. :)

    >>> os.mkdir('empty')
    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Prefix duplication
==================

Sometimes, if you've been very bad, you may need to duplicate keys.
In a web server, you might hace rules to rewrite URLs.

You can provide some prefix pairs on the command line. When adding or
removing keys, you can add and remove duplicate paths starting with a
given prefix to another prefix.

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/',
    ...    ])

Here we duplicated everything to dup1 and dup2, and
duplicates everything in d1/d2 to a top level d2 directory.

    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    d2/f1
    d2/f3
    dup/d1/d2/f1
    dup/d1/d2/f3
    dup/d1/f1
    dup/d1/f3
    dup/f1
    dup/f3
    dup2/d1/d2/f1
    dup2/d1/d2/f3
    dup2/d1/f1
    dup2/d1/f3
    dup2/f1
    dup2/f3
    f1
    f3

    >>> for k in bucket.list('dup/'):
    ...     k.check('sample', 'dup/')

    >>> for k in bucket.list('dup2/'):
    ...     k.check('sample', 'dup2/')

    >>> for k in bucket.list('d1/'):
    ...     k.check('sample')

Do it again, for good measure:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/',
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    d2/f1
    d2/f3
    dup/d1/d2/f1
    dup/d1/d2/f3
    dup/d1/f1
    dup/d1/f3
    dup/f1
    dup/f3
    dup2/d1/d2/f1
    dup2/d1/d2/f3
    dup2/d1/f1
    dup2/d1/f3
    dup2/f1
    dup2/f3
    f1
    f3

Cleanup:

    >>> zc.s3staticsync.main(
    ...       ['empty', 'test',
    ...       '=dup/', '=dup2/', 'd1/d2/=d2/',
    ...       ])
    >>> list(bucket)
    []

Important note!
  If you change the prefix-rewriting configuration, it will only be
  honored for new changes.

Avoiding deletes
================

If you supply the -D option, no keys in S3 will be deleted:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test',
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3


    >>> now += 3000
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')
    >>> os.remove('sample/f3')
    >>> os.remove('sample/d1/f3')
    >>> os.remove('sample/d1/d2/f3')


    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test', '-D'
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f2
    d1/d2/f3
    d1/f1
    d1/f2
    d1/f3
    f1
    f2
    f3

    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Bucket prefixes
===============

If the bucket name has a '/' in it, then the text after the '/' is
added as a prefix for each item.

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/',
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f2
    x/d1/f1
    x/d1/f2
    x/d2/f1
    x/d2/f2
    x/dup/d1/d2/f1
    x/dup/d1/d2/f2
    x/dup/d1/f1
    x/dup/d1/f2
    x/dup/f1
    x/dup/f2
    x/dup2/d1/d2/f1
    x/dup2/d1/d2/f2
    x/dup2/d1/f1
    x/dup2/d1/f2
    x/dup2/f1
    x/dup2/f2
    x/f1
    x/f2

    >>> zc.s3staticsync.main(
    ...    ['empty', 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/',
    ...    ])
    >>> list(bucket)
    []

Using a local index
-------------------

Listing a large S3 bucket can be expensive.  To avoid this expense, we
can create a local index of files that lists every file and it's
modification time as of the last time we synced.

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/', '-iindex'
    ...    ])

Now, we have an index file:

    >>> import marshal, pprint
    >>> with open('index') as f:
    ...     pprint.pprint(marshal.load(f))
    {u'd1/d2/f1': 1379910262,
     u'd1/d2/f2': 1379916862,
     u'd1/f1': 1379903832,
     u'd1/f2': 1379916862,
     u'f1': 1379903832,
     u'f2': 1379916862}

Because the index didn't exist, we still listed the bucket:

    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 20, 0)

If we started with an index and sync in the same second that files are
written, we sleep a second to make sure we don't miss updates.  Since
we didn't have an index, we didn't sleep:

    >>> now == before
    True

Now, we'll make some changes:

    >>> now += 3600
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')
    >>> now += 3600

Now when we sync, we'll use the index:

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/', '-iindex'
    ...    ])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 14, 10)

The bucket wasn't consulted, but we still did the right updates:

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/f1
    x/d1/f3
    x/d2/f1
    x/d2/f3
    x/dup/d1/d2/f1
    x/dup/d1/d2/f3
    x/dup/d1/f1
    x/dup/d1/f3
    x/dup/f1
    x/dup/f3
    x/dup2/d1/d2/f1
    x/dup2/d1/d2/f3
    x/dup2/d1/f1
    x/dup2/d1/f3
    x/dup2/f1
    x/dup2/f3
    x/f1
    x/f3

Because we synced long after updating the files, we didn't sleep:

    >>> now == before
    True

Now, we'll update a file and sync again:

    >>> mkfile('sample/f3')
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/', '-iindex'
    ...    ])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 3, 0)

Because we synced in the same second we write the file, we slept a
second:

    >>> 0.9 < (now - before) < 1.1
    True

At this point, the index has been updated, so if we sync again, there
won't be any changes:

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/',
    ...    '=dup/', '=dup2/', 'd1/d2/=d2/', '-iindex'
    ...    ])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 0)
