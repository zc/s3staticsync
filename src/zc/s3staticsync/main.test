S3static sync tests
===================

We have a sample directory and a test bucket (in a faux s3).

    >>> import os, mock
    >>> os.mkdir('sample')
    >>> mkfile('sample/f1')
    >>> mkfile('sample/d1/f1')
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')

Cuz we're mean, we'll create a broken symlink:

    >>> os.symlink('lose', 'sample/d1/hahaha')

Later:

    >>> now += 3600

We'll sync to our faux bucket:

    >>> import zc.s3staticsync

    >>> now += 30
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    ... # doctest: +ELLIPSIS
    bad file 'd1/hahaha'
    Traceback (most recent call last):
    ...
    OSError: [Errno 2] No such file or directory: '.../sample/d1/hahaha'

Note that we logged an exception for the bad link, but we kept going.

    >>> os.remove('sample/d1/hahaha')

Let's check what we have in our bucket:

    >>> import boto.s3.connection
    >>> s3 = boto.s3.connection.S3Connection()
    >>> bucket = s3.get_bucket('test')

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f2
    d1/f1
    d1/f2
    f1
    f2

We did 6 puts and no deletes:

    >>> bucket.puts, bucket.deletes
    (6, 0)

Time passes and we so it all again:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

Nothing should have been put or deleted:

    >>> bucket.puts, bucket.deletes
    (6, 0)


 Let's make some changes and make sure they're reflected:

    >>> now += 1000
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')

    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3

    >>> bucket.puts, bucket.deletes
    (10, 3)

There's a kinda weird case.  We compare file modification time to s3
modification time. To account for that, and for clocks being out of
sync, we add a fudge factor to the modification time. This causes
newly modified files to be uploaded twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

But only twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

Emptying a bucket
=================

An easy way to empty a bucket is to sync from an empty directory. :)

    >>> os.mkdir('empty')
    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Avoiding deletes
================

If you supply the -D option, no keys in S3 will be deleted:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test',
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3


    >>> now += 3000
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')
    >>> os.remove('sample/f3')
    >>> os.remove('sample/d1/f3')
    >>> os.remove('sample/d1/d2/f3')


    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test', '-D'
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f2
    d1/d2/f3
    d1/f1
    d1/f2
    d1/f3
    f1
    f2
    f3

    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Bucket prefixes
===============

If the bucket name has a '/' in it, then the text after the '/' is
added as a prefix for each item.

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/'])
    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f2
    x/d1/f1
    x/d1/f2
    x/f1
    x/f2

    >>> zc.s3staticsync.main(['empty', 'test/x/'])
    >>> list(bucket)
    []

Using a local index
-------------------

Listing a large S3 bucket can be expensive.  To avoid this expense, we
can create a local index of files that lists every file and it's
modification time as of the last time we synced.

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])

Now, we have an index file:

    >>> import marshal, pprint
    >>> with open('index') as f:
    ...     pprint.pprint(marshal.load(f))
    {u'd1/d2/f1': 1379910262,
     u'd1/d2/f2': 1379916862,
     u'd1/f1': 1379903832,
     u'd1/f2': 1379916862,
     u'f1': 1379903832,
     u'f2': 1379916862}

Because the index didn't exist, we still listed the bucket:

    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 6, 0)

If we started with an index and sync in the same second that files are
written, we sleep a second to make sure we don't miss updates.  Since
we didn't have an index, we didn't sleep:

    >>> now == before
    True

Now, we'll make some changes:

    >>> now += 3600
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')
    >>> now += 3600

Now when we sync, we'll use the index:

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex'
    ...    ])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 4, 3)

The bucket wasn't consulted, but we still did the right updates:

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/f1
    x/d1/f3
    x/f1
    x/f3

Because we synced long after updating the files, we didn't sleep:

    >>> now == before
    True

Now, we'll update a file and sync again:

    >>> mkfile('sample/f3')
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 1, 0)

Because we synced in the same second we write the file, we slept a
second:

    >>> 0.9 < (now - before) < 1.1
    True

At this point, the index has been updated, so if we sync again, there
won't be any changes:

    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 0)

Sometimes, we want to read S3, to make sure we're in sync, but we
still want to update the index.  We can add the -I option for this:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-I'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 1, 0)

(Note that we uploaded f3 again because, without a local index, we use
a fudge factor and are a lot more conservative about uploading things.)

Locking
=======

If running this from cron, you probably don't want runs to overlap.
To avoid this, you can specify a lock file using the -l option:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])

    >>> os.path.exists('lock')
    True

    >>> with open('lock') as f:
    ...     pid = int(f.read().strip())
    >>> pid == os.getpid()
    True

If we lock ourselves

    >>> import zc.lockfile
    >>> lock =  zc.lockfile.LockFile('lock')

Then we'll get an error is we try to sync:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])
    Traceback (most recent call last):
       ...
    LockError: Couldn't lock 'lock'

    >>> lock.close()

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])

S3 Errors
=========

When using an index, if there's an error uploading to S3:

- The upload will be retried once after a 9 second sleep.

- If an upload fails a second time, the index won't have an entry for
  the document, causing further attemps on the next sync.

    >>> bucket.fail = True
    >>> mkfile('sample/d1/d2/f1')

    >>> now += 99
    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    uploading 1379924063 ...d1/d2/f1', retrying
    Traceback (most recent call last):
    ...
    ValueError: fail
    processing 1379924063 ...sample/d1/d2/f1'
    Traceback (most recent call last):
    ...
    ValueError: fail

    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 2, 0)

    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    False

    >>> 8.9 < (now - before) < 9.1
    True

Later, hopefully, the upload will succeed:

    >>> bucket.fail = False
    >>> now += 99
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 1, 0)
    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    True

Same drill with deletes:

    >>> bucket.fail = True
    >>> os.remove('sample/d1/d2/f1')
    >>> now += 99
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> before = now
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    deleting u'x/d1/d2/f1', retrying
    Traceback (most recent call last):
    ...
    ValueError: fail
    processing None u'd1/d2/f1'
    Traceback (most recent call last):
    ...
    ValueError: fail

    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 2)
    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    True
    >>> 8.9 < (now - before) < 9.1
    True
