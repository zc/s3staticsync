S3static sync tests
===================

We have a sample directory and a test bucket (in a faux s3).

    >>> import os, mock
    >>> os.mkdir('sample')
    >>> mkfile('sample/f1')
    >>> mkfile('sample/d1/f1')
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')

Cuz we're mean, we'll create a broken symlink:

    >>> os.symlink('lose', 'sample/d1/hahaha')

Later:

    >>> now += 3600

We'll sync to our faux bucket:

    >>> import zc.s3staticsync

    >>> now += 30
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    ... # doctest: +ELLIPSIS
    bad file 'd1/hahaha'
    Traceback (most recent call last):
    ...
    OSError: [Errno 2] No such file or directory: '.../sample/d1/hahaha'

Note that we logged an exception for the bad link, but we kept going.

    >>> os.remove('sample/d1/hahaha')

Let's check what we have in our bucket:

    >>> import boto.s3.connection
    >>> s3 = boto.s3.connection.S3Connection()
    >>> bucket = s3.get_bucket('test')

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f2
    d1/f1
    d1/f2
    f1
    f2

We did 6 puts and no deletes:

    >>> bucket.puts, bucket.deletes
    (6, 0)

Time passes and we so it all again:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

Nothing should have been put or deleted:

    >>> bucket.puts, bucket.deletes
    (6, 0)


 Let's make some changes and make sure they're reflected:

    >>> now += 1000
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')

    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])

    >>> for k in bucket:
    ...     print k.key
    ...     k.check('sample')
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3

    >>> bucket.puts, bucket.deletes
    (10, 3)

There's a kinda weird case.  We compare file modification time to s3
modification time. To account for that, and for clocks being out of
sync, we add a fudge factor to the modification time. This causes
newly modified files to be uploaded twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

But only twice:

    >>> now += 1800
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test'])
    >>> bucket.puts, bucket.deletes
    (14, 3)

Emptying a bucket
=================

An easy way to empty a bucket is to sync from an empty directory. :)

    >>> os.mkdir('empty')
    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Avoiding deletes
================

If you supply the -D option, no keys in S3 will be deleted:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test',
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f3
    d1/f1
    d1/f3
    f1
    f3


    >>> now += 3000
    >>> mkfile('sample/f2')
    >>> mkfile('sample/d1/f2')
    >>> mkfile('sample/d1/d2/f2')
    >>> os.remove('sample/f3')
    >>> os.remove('sample/d1/f3')
    >>> os.remove('sample/d1/d2/f3')


    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test', '-D'
    ...    ])
    >>> for k in bucket:
    ...     print k.key
    d1/d2/f1
    d1/d2/f2
    d1/d2/f3
    d1/f1
    d1/f2
    d1/f3
    f1
    f2
    f3

    >>> zc.s3staticsync.main(['empty', 'test'])
    >>> list(bucket)
    []

Bucket prefixes
===============

If the bucket name has a '/' in it, then the text after the '/' is
added as a prefix for each item.

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/'])
    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f2
    x/d1/f1
    x/d1/f2
    x/f1
    x/f2

    >>> zc.s3staticsync.main(['empty', 'test/x/'])
    >>> list(bucket)
    []

Using a local index
-------------------

Listing a large S3 bucket can be expensive.  To avoid this expense, we
can create a local index of files that lists every file and it's
modification time as of the last time we synced.

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])

Now, we have an index file:

    >>> import marshal, pprint
    >>> with open('index') as f:
    ...     pprint.pprint(marshal.load(f))
    {u'd1/d2/f1': 1379910262,
     u'd1/d2/f2': 1379916862,
     u'd1/f1': 1379903832,
     u'd1/f2': 1379916862,
     u'f1': 1379903832,
     u'f2': 1379916862}

Because the index didn't exist, we still listed the bucket:

    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 6, 0)

If we started with an index and sync in the same second that files are
written, we sleep a second to make sure we don't miss updates.  Since
we didn't have an index, we didn't sleep:

    >>> now == before
    True

Now, we'll make some changes:

    >>> now += 3600
    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/f3')
    >>> mkfile('sample/d1/f3')
    >>> mkfile('sample/d1/d2/f3')

    >>> os.remove('sample/f2')
    >>> os.remove('sample/d1/f2')
    >>> os.remove('sample/d1/d2/f2')
    >>> now += 3600

Now when we sync, we'll use the index:

    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex'
    ...    ])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 4, 3)

The bucket wasn't consulted, but we still did the right updates:

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/f1
    x/d1/f3
    x/f1
    x/f3

Because we synced long after updating the files, we didn't sleep:

    >>> now == before
    True

Now, we'll update a file and sync again:

    >>> mkfile('sample/f3')
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 1, 0)

Because we synced in the same second we write the file, we slept a
second:

    >>> 0.9 < (now - before) < 1.1
    True

At this point, the index has been updated, so if we sync again, there
won't be any changes:

    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main([os.path.abspath('sample'), 'test/x/', '-iindex'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 0)

Sometimes, we want to read S3, to make sure we're in sync, but we
still want to update the index.  We can add the -I option for this:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-I'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 1, 0)

(Note that we uploaded f3 again because, without a local index, we use
a fudge factor and are a lot more conservative about uploading things.)

Locking
=======

If running this from cron, you probably don't want runs to overlap.
To avoid this, you can specify a lock file using the -l option:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])

    >>> os.path.exists('lock')
    True

    >>> with open('lock') as f:
    ...     pid = int(f.read().strip())
    >>> pid == os.getpid()
    True

If we lock ourselves

    >>> import zc.lockfile
    >>> lock =  zc.lockfile.LockFile('lock')

Then we'll get an error is we try to sync:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])
    Traceback (most recent call last):
       ...
    LockError: Couldn't lock 'lock'

    >>> lock.close()

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock'])

S3 Errors
=========

When using an index, if there's an error uploading to S3:

- The upload will be retried once after a 9 second sleep.

- If an upload fails a second time, the index won't have an entry for
  the document, causing further attemps on the next sync.

    >>> bucket.fail = True
    >>> mkfile('sample/d1/d2/f1')

    >>> now += 99
    >>> before = now
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    uploading 1379924063 ...d1/d2/f1', retrying
    Traceback (most recent call last):
    ...
    ValueError: fail
    processing 1379924063 ...sample/d1/d2/f1'
    Traceback (most recent call last):
    ...
    ValueError: fail

    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 2, 0)

    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    False

    >>> 8.9 < (now - before) < 9.1
    True

Later, hopefully, the upload will succeed:

    >>> bucket.fail = False
    >>> now += 99
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 1, 0)
    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    True

Same drill with deletes:

    >>> bucket.fail = True
    >>> os.remove('sample/d1/d2/f1')
    >>> now += 99
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> before = now
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS
    deleting u'x/d1/d2/f1', retrying
    Traceback (most recent call last):
    ...
    ValueError: fail
    processing None u'd1/d2/f1'
    Traceback (most recent call last):
    ...
    ValueError: fail

    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 2)
    >>> with open('index') as f:
    ...     print 'd1/d2/f1' in marshal.load(f)
    True
    >>> 8.9 < (now - before) < 9.1
    True

    >>> bucket.fail = False
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock',
    ...    ]) # doctest: +ELLIPSIS

index.html generation
=====================

If you're migrating from an apache web server to S3, you may want
automatically generated index files.  In directories that already have
index files, we want them to be retained.  We'll create an index
manually to show that it isn't overwritten:

    >>> mkfile('sample/d1/index.html', 'manual index')

Now, we'll sync again, but this time, we'll ask (-g) for index.html files
to be generated:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g'])
    >>> now += 3000

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f3
    x/d1/d2/index.html
    x/d1/f1
    x/d1/f3
    x/d1/index.html
    x/f1
    x/f3

Notice we got an index file generated in d1/d2.

    >>> print bucket.data['x/d1/d2/index.html'][0],
    <!-- generated -->
    <html><head><title>Index of d1/d2</title></head><body>
    <h1>Index of d1/d2</h1><table>
    <tr><th>Name</th><th>Last modified</th><th>Size</th></tr>
    <tr><td><a href="f3">f3</a></td>
        <td>Sun Sep 22 22:14:22 2013</td><td>40726</td></tr>
    </table></body></html>

Our original index is preserved:

    >>> print bucket.data['x/d1/index.html'][0]
    manual index

If we add a file and a directory to d1/d2, it will be reflected in the index:

    >>> mkfile('sample/d1/d2/f1')
    >>> mkfile('sample/d1/d2/d3/f1')
    >>> now += 3000
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g'])

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/d3/f1
    x/d1/d2/d3/index.html
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/d2/index.html
    x/d1/f1
    x/d1/f3
    x/d1/index.html
    x/f1
    x/f3

Notice we got an index file generated in d1/d2.

    >>> print bucket.data['x/d1/d2/index.html'][0], # doctest: +ELLIPSIS
    <!-- generated -->
    <html><head><title>Index of d1/d2</title></head><body>
    <h1>Index of d1/d2</h1><table>
    <tr><th>Name</th><th>Last modified</th><th>Size</th></tr>
    <tr><td><a href="d3/">d3/</a></td>
        <td>...</td><td>-</td></tr>
    <tr><td><a href="f1">f1</a></td>
        <td>Mon Sep 23 00:09:39 2013</td><td>50628</td></tr>
    <tr><td><a href="f3">f3</a></td>
        <td>Sun Sep 22 22:14:22 2013</td><td>40726</td></tr>
    </table></body></html>

Our original index is still preserved:

    >>> print bucket.data['x/d1/index.html'][0]
    manual index

If we remove a directory, the generated index file will be removed from s3:

    >>> os.remove('sample/d1/d2/d3/f1')
    >>> os.rmdir('sample/d1/d2/d3')
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g'])

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/d2/index.html
    x/d1/f1
    x/d1/f3
    x/d1/index.html
    x/f1
    x/f3

We can replace generated indexes:

    >>> mkfile('sample/d1/d2/index.html', 'manual index d1/d2')
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g',
    ...    ]) # doctest: +ELLIPSIS
    >>> print bucket.data['x/d1/d2/index.html'][0],
    manual index d1/d2

    >>> os.remove('sample/d1/d2/index.html')
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g',
    ...    ]) # doctest: +ELLIPSIS
    >>> print bucket.data['x/d1/d2/index.html'][0], # doctest: +ELLIPSIS
    <!-- generated -->
    <html><head><title>Index of d1/d2</title></head><body>
    <h1>Index of d1/d2</h1><table>
    ...

Because we're using an file index, we can avoid uploading a generated
index is it hasn't changed. This is important for avoiding S3
charges. Let's verify this:

    >>> now += 99
    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-g',
    ...    ]) # doctest: +ELLIPSIS
    >>> bucket.listed, bucket.puts, bucket.deletes
    (0, 0, 0)


We can ignore the existing index:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-iindex', '-llock', '-gI'])

    >>> for k in bucket:
    ...     print k.key
    x/d1/d2/f1
    x/d1/d2/f3
    x/d1/d2/index.html
    x/d1/f1
    x/d1/f3
    x/d1/index.html
    x/f1
    x/f3

    >>> print bucket.data['x/d1/index.html'][0]
    manual index

    >>> print bucket.data['x/d1/d2/index.html'][0], # doctest: +ELLIPSIS
    <!-- generated -->
    <html><head><title>Index of d1/d2</title></head><body>
    <h1>Index of d1/d2</h1><table>
    ...

Or we can avoid using an index altogether:

    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-g'])

    >>> now += 3000

However, since we're ignoring the index, we'll end up putting the
index to S3:

    >>> bucket.puts = bucket.deletes = bucket.listed = 0
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-g'])
    >>> bucket.listed, bucket.puts, bucket.deletes
    ('x/', 1, 0)

Let's place a generated w static and static w generated and make sure
we still get the correct behavior:

    >>> mkfile('sample/d1/d2/index.html', 'index of d1/d2')
    >>> os.remove('sample/d1/index.html')
    >>> zc.s3staticsync.main(
    ...    [os.path.abspath('sample'), 'test/x/', '-g'])

    >>> print bucket.data['x/d1/index.html'][0], # doctest: +ELLIPSIS
    <!-- generated -->
    <html><head><title>Index of d1</title></head><body>
    <h1>Index of d1</h1><table>
    <tr><th>Name</th><th>Last modified</th><th>Size</th></tr>
    <tr><td><a href="d2/">d2/</a></td>
        <td>...</td><td>-</td></tr>
    <tr><td><a href="f1">f1</a></td>
        <td>Sun Sep 22 17:37:12 2013</td><td>13337</td></tr>
    <tr><td><a href="f3">f3</a></td>
        <td>Sun Sep 22 22:14:22 2013</td><td>47519</td></tr>
    </table></body></html>

    >>> print bucket.data['x/d1/d2/index.html'][0], # doctest: +ELLIPSIS
    index of d1/d2
